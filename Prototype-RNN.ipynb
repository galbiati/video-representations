{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "import load_data as load\n",
    "from models.model import Model\n",
    "from models.customlayers import *\n",
    "from models.activations import *\n",
    "from training import *\n",
    "\n",
    "import moviepy.editor as mpe\n",
    "from models.AELSTM import *\n",
    "L = tf.layers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = os.path.expanduser('~/Insight/video-representations/frames')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AE (No LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of a new variable (decoder/dense1/kernel) must be fully defined, but instead was <unknown>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4bb8d6a17e2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtraining_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_encoder_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# discard decoder here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtraining_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Insight/video-representations/models/model.py\u001b[0m in \u001b[0;36mbuild_encoder_decoder\u001b[0;34m(self, inputs, reuse)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'decoder'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mdecoded\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0mdecoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Insight/video-representations/models/AELSTM.py\u001b[0m in \u001b[0;36mtied_decoder\u001b[0;34m(encoded)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \"\"\"\n\u001b[1;32m     67\u001b[0m     \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dense1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dense1/kernel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mw_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dense1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mdefault_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m   1063\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m       use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m   1066\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1067\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m    960\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 962\u001b[0;31m           use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m           validate_shape=validate_shape, use_resource=use_resource)\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource)\u001b[0m\n\u001b[1;32m    350\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m           use_resource=use_resource)\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource)\u001b[0m\n\u001b[1;32m    683\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fully_defined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minitializing_from_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m       raise ValueError(\"Shape of a new variable (%s) must be fully defined, \"\n\u001b[0;32m--> 685\u001b[0;31m                        \"but instead was %s.\" % (name, shape))\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0;31m# Create the tensor to initialize the variable with default value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of a new variable (decoder/dense1/kernel) must be fully defined, but instead was <unknown>."
     ]
    }
   ],
   "source": [
    "# check no-lstm AE\n",
    "\n",
    "training_epochs = 10\n",
    "batchsize = 4\n",
    "sequence_length = 64\n",
    "\n",
    "model = Model(encoder, lstm_cell, decoder, batchsize, sequence_length)\n",
    "\n",
    "## LSTM-Encoder Training Graph ##\n",
    "\n",
    "training_inputs, training_targets = load.inputs('training', batchsize, training_epochs)\n",
    "\n",
    "encoded, decoded = model.build_encoder_decoder(training_inputs, reuse=False)    # discard decoder here\n",
    "loss = tf.reduce_mean(tf.pow(decoded - training_targets, 2))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "trainable_vars = tf.trainable_variables()\n",
    "clipped_gradients, _ = tf.clip_by_global_norm(tf.gradients(loss, trainable_vars), 1)    # clip those uglies\n",
    "train_step = optimizer.apply_gradients(zip(clipped_gradients, trainable_vars))\n",
    "\n",
    "## LSTM-Encoder Validation Graph ##\n",
    "\n",
    "validation_inputs, validation_targets = load.inputs('validation', batchsize, 1)\n",
    "\n",
    "encoded_validation, decoded_validation = model.build_encoder_decoder(validation_inputs, reuse=True)\n",
    "validation_loss = tf.reduce_mean(tf.pow(decoded_validation - validation_targets, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "init_global = tf.global_variables_initializer()\n",
    "init_local = tf.local_variables_initializer()\n",
    "\n",
    "coord = tf.train.Coordinator()\n",
    "\n",
    "with tf.Session() as sesh:\n",
    "    sesh.run([init_global, init_local])\n",
    "    threads = tf.train.start_queue_runners(sess=sesh, coord=coord)\n",
    "    \n",
    "    # initialize lists for tracking\n",
    "    \n",
    "    decoder_losses = []\n",
    "    decoder_validation_losses = []\n",
    "    \n",
    "    predictions = []\n",
    "    encodings = []\n",
    "    validation_predictions = []\n",
    "    validation_encodings = []\n",
    "    recovery = []\n",
    "    validation_recovery = []\n",
    "    \n",
    "    # first, encoder training\n",
    "    try:\n",
    "        step = 0\n",
    "        \n",
    "        while not coord.should_stop():\n",
    "            _, loss_value, enc, pred, input_recover = sesh.run(\n",
    "                [train_step, loss, encoded, decoded, training_targets]\n",
    "            )\n",
    "            \n",
    "            decoder_losses.append(loss_value)\n",
    "            \n",
    "            if step % 250 == 0:\n",
    "                print(step, loss_value)\n",
    "                encodings.append(enc)\n",
    "                predictions.append(pred)\n",
    "                recovery.append(input_recover)\n",
    "                \n",
    "            step += 1\n",
    "            \n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('Encoder trained: {:.2f}'.format(loss_value))\n",
    "        \n",
    "    # second, encoder validation\n",
    "    try:\n",
    "        step = 0\n",
    "        \n",
    "        while not coord.should_stop():\n",
    "            loss_value, enc, pred, input_recover = sesh.run(\n",
    "                [validation_loss, encoded_validation, decoded_validation, validation_targets]\n",
    "            )\n",
    "            decoder_validation_losses.append(loss_value)\n",
    "            \n",
    "            if step % 100 == 0:\n",
    "                print(step, loss_value)\n",
    "                validation_encodings.append(enc)\n",
    "                validation_predictions.append(pred)\n",
    "                validation_recovery.append(input_recover)\n",
    "                \n",
    "            step += 1\n",
    "            \n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('Encoder validated: {:.2f}'.format(loss_value))\n",
    "        \n",
    "    finally:\n",
    "        coord.request_stop()\n",
    "        \n",
    "    coord.join(threads)\n",
    "    saver.save(sesh, 'AE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM-AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder/Reshape:0\n",
      "[5, 5, 32, 64] [512, 48, 68, 64]\n",
      "[5, 5, 32, 32] [512, 52, 72, 32]\n",
      "[3, 3, 16, 32] [512, 56, 76, 32]\n",
      "[3, 3, 3, 16] [512, 58, 78, 16]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "trainable_variables() got an unexpected keyword argument 'learning_rate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-10db3d1c7078>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtrainable_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.00001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mclipped_gradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_by_global_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# clip those uglies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mtrain_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipped_gradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: trainable_variables() got an unexpected keyword argument 'learning_rate'"
     ]
    }
   ],
   "source": [
    "training_epochs = 20\n",
    "batchsize = 8\n",
    "sequence_length = 64\n",
    "\n",
    "model = Model(encoder, lstm_cell, tied_decoder, batchsize, sequence_length)\n",
    "\n",
    "## LSTM-Encoder Training Graph ##\n",
    "\n",
    "training_inputs, training_targets = load.inputs('training', batchsize, training_epochs)\n",
    "\n",
    "encoded, transitioned, decoded = model.build(training_inputs)    # discard decoder here\n",
    "loss = tf.reduce_mean(tf.pow(decoded - training_targets, 2))\n",
    "\n",
    "# decoded_ = model.build_encoder_decoder(encoded, reuse=True)\n",
    "    \n",
    "# encoded_loss = tf.reduce_mean(tf.pow(decoded_ - training_inputs, 2))\n",
    "\n",
    "# loss = transitioned_loss + encoded_loss\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=.00001)\n",
    "trainable_vars = tf.trainable_variables()\n",
    "clipped_gradients, _ = tf.clip_by_global_norm(tf.gradients(loss, trainable_vars), 1)    # clip those uglies\n",
    "train_step = optimizer.apply_gradients(zip(clipped_gradients, trainable_vars))\n",
    "\n",
    "## LSTM-Encoder Validation Graph ##\n",
    "\n",
    "validation_inputs, validation_targets = load.inputs('validation', batchsize, 1)\n",
    "\n",
    "encoded_validation, transitioned_validation, decoded_validation = model.build(validation_inputs, reuse=True)\n",
    "validation_loss = tf.reduce_mean(tf.pow(decoded_validation - validation_targets, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220.70499799999999"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()]) / 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 18543.9\n",
      "250 3947.64\n",
      "500 3516.28\n",
      "750 3002.4\n",
      "1000 2474.34\n",
      "1250 1864.75\n",
      "1500 2147.64\n",
      "1750 1947.26\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "init_global = tf.global_variables_initializer()\n",
    "init_local = tf.local_variables_initializer()\n",
    "\n",
    "coord = tf.train.Coordinator()\n",
    "\n",
    "with tf.Session() as sesh:\n",
    "    sesh.run([init_global, init_local])\n",
    "    threads = tf.train.start_queue_runners(sess=sesh, coord=coord)\n",
    "    \n",
    "    # initialize lists for tracking\n",
    "    \n",
    "    decoder_losses = []\n",
    "    decoder_validation_losses = []\n",
    "    \n",
    "    predictions = []\n",
    "    encodings = []\n",
    "    transitions = []\n",
    "    validation_predictions = []\n",
    "    validation_transitions = []\n",
    "    validation_encodings = []\n",
    "    recovery = []\n",
    "    validation_recovery = []\n",
    "    \n",
    "    # first, encoder training\n",
    "    try:\n",
    "        step = 0\n",
    "        \n",
    "        while not coord.should_stop():\n",
    "            _, loss_value, enc, trans, pred, input_recover = sesh.run(\n",
    "                [train_step, loss, encoded, transitioned, decoded, training_targets]\n",
    "            )\n",
    "            \n",
    "            decoder_losses.append(loss_value)\n",
    "            \n",
    "            if step % 250 == 0:\n",
    "                print(step, loss_value)\n",
    "                encodings.append(enc)\n",
    "                transitions.append(trans)\n",
    "                predictions.append(pred)\n",
    "                recovery.append(input_recover)\n",
    "                \n",
    "            step += 1\n",
    "            \n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('Encoder trained: {:.2f}'.format(loss_value))\n",
    "        \n",
    "    # second, encoder validation\n",
    "    try:\n",
    "        step = 0\n",
    "        \n",
    "        while not coord.should_stop():\n",
    "            loss_value, enc, trans, pred, input_recover = sesh.run(\n",
    "                [validation_loss, encoded_validation, transitioned_validation, \n",
    "                 decoded_validation, validation_targets]\n",
    "            )\n",
    "            decoder_validation_losses.append(loss_value)\n",
    "            \n",
    "            if step % 100 == 0:\n",
    "                print(step, loss_value)\n",
    "                validation_encodings.append(enc)\n",
    "                validation_transitions.append(trans)\n",
    "                validation_predictions.append(pred)\n",
    "                validation_recovery.append(input_recover)\n",
    "                \n",
    "            step += 1\n",
    "            \n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('Encoder validated: {:.2f}'.format(loss_value))\n",
    "        \n",
    "    finally:\n",
    "        coord.request_stop()\n",
    "        \n",
    "    coord.join(threads)\n",
    "    saver.save(sesh, 'ptypelstm-tied-relu')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(10, 10))\n",
    "def rolling_mean(l, w=500):\n",
    "    c = np.cumsum(l)\n",
    "    c[w:] = c[w:] - c[:-w]\n",
    "    return c[w-1:] / w\n",
    "    \n",
    "\n",
    "axes.plot(decoder_losses)\n",
    "axes.plot(rolling_mean(decoder_losses))\n",
    "plt.setp(axes, ylim=[0, 2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 6))\n",
    "vid = -2\n",
    "fid = 3\n",
    "axes[0].imshow(np.minimum(np.maximum(predictions[vid][0, fid, :, :, :], 0), 255) / 255)\n",
    "axes[1].imshow(np.minimum(np.maximum(recovery[vid][0, fid+1, :, :, :], 0), 255) / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encodings[-1][0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transitions[-1][0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from render import render_movie\n",
    "\n",
    "frame_array = render_movie(recovery[vid][0], 'test_ae_recov2.mp4', 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
